{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ! git clone https://github.com/build1024/SeqGAN.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ! pip install cshogi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 概要\n- 正例：学習済み言語モデルからサンプリングした文章（元データ）\n- 負例：Pre-trainingしたGeneratorが出力した文章\n\n- 参考：https://github.com/X-czh/SeqGAN-PyTorch\n\n## Pretrain\n\n- Generator\n    - 入力：正例\n    - 出力(softmax)：単語ID\n    - cross_ent：正例の単語IDと出力の単語ID\n    - このとき負例を生成し保存\n    \n- Discriminator\n    - 入力：正例と負例\n    - 出力(softmax)：正例(1)or負例(0)\n    - 入力が正例のとき1、負例のとき0になるよう学習させる\n    \n## main train\nGeneratorに文章をいくつか作らせてみて(pretrain)、Discriminatorによる評価結果をGeneratorに与えて学習させる\n    \n- Generator\n    - 入力：負例\n    - 出力：単語ID\n    - pg_loss：rolloutから報酬を取得し、報酬をもとに損失値を求める\n\n- Discriminator\n    - 入力：正例と負例\n    - 出力(softmax)：正例(1)or負例(0)\n    - 入力が正例のとき1、負例のとき0になるよう学習させる","metadata":{}},{"cell_type":"markdown","source":"## 棋譜->数値データ","metadata":{}},{"cell_type":"code","source":"# import cshogi\n# import numpy as np\n# data_file =\"../input/mate-shogi/mate3.sfen\"\n# output_file = \"../input/mate-shogi/mate3.txt\"\n\n# with open(data_file, 'r') as f:\n# #     lines = f.readlines()\n#     lis = []\n#     for i, line in enumerate(f):\n#         #fsen->board\n#         l = line.replace(\"\\n\", \"\")\n#         board = cshogi.Board(l)\n#         #board->hcps\n#         hcps = np.empty(1, dtype=cshogi.HuffmanCodedPos)\n#         board.to_hcp(hcps)\n\n#         lis.append(hcps[0][0])\n\n# lis = np.array(lis)\n\n# np.savetxt(output_file, lis)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# l = np.loadtxt(output_file, dtype=cshogi.HuffmanCodedPos)\n# l.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with open(data_file, 'r') as f:\n#     lines = f.readlines()\n#     lis = []\n# for i, line in enumerate(lines):\n#     l = line.replace(\"\\n\", \"\")\n#     board = cshogi.Board(l)\n\n#     display(board)\n#     break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for h in lis:\n#     board = cshogi.Board()\n#     print(h)\n#     print(lis)\n#     board.set_hcp(h)\n#     display(board)\n#     break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for h in lis[:5]:\n#     print(h)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #fsen->hcpsに変換（数値化）\n# hcps = np.empty(1, dtype=cshogi.HuffmanCodedPos)\n# board.to_hcp(hcps)\n# # hcps.tofile('hcp')\n# print(hcps)\n# #hcps->将棋盤に変換\n# board = cshogi.Board()\n# board.set_hcp(hcps)\n# board","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# l = np.loadtxt(\"../input/mate-shogi/mate3.txt\")\n# l","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# l.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# l[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# l = np.loadtxt(\"../input/mate-shogi/mate3.txt\", dtype=np.int32)\n# print(l)\n# print(l.shape)\n# print(l[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vocab = tuple()\n# for s in l:\n#     print(tuple(s))\n#     print(vocab+tuple(s))\n#     break\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# l = l.reshape(998405*32)\n# l.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle as pkl\nimport math\nimport random\nimport copy\nimport os\nimport numpy as np\n# import cshogi\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\n\n\n# Files\nPOSITIVE_FILE = \"mate3.txt\"\nNEGATIVE_FILE = 'gene.data'\nhpc = False\ndata_path = \"../input/mate-shogi\"\n\nrounds = 50\ng_pretrain_steps = 10\nd_pretrain_steps = 10\ng_steps = 1\nd_steps = 3\ngk_epochs = 3\ngk_epochs = 1\ndk_epochs = 3\nupdate_rate = 0.8\nn_rollout = 16\nvocab_size = 256 #len(set(data = mate3.txt))\nbatch_size = 64\nn_samples = batch_size*100\ngen_lr = 1e-3\ndis_lr = 1e-3\nno_cuda = False\nseed = 1\n\n\n# Genrator Parameters\ng_embed_dim = 32\ng_hidden_dim = 32\ng_seq_len = 32\n\n\n# Discriminator Parameters\nd_num_class = 2\nd_embed_dim = 64\nd_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\nd_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\nd_dropout_prob = 0.2\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"class GenDataIter:\n    \"\"\" Toy data iter to load digits \"\"\"\n\n    def __init__(self, data_file, batch_size):\n        super(GenDataIter, self).__init__()\n        self.batch_size = batch_size\n        self.data_lis = self.read_file(data_file)\n        self.data_num = len(self.data_lis)\n        self.indices = range(self.data_num)\n        self.num_batches = math.ceil(self.data_num / self.batch_size)\n        self.idx = 0\n        self.reset()\n\n    def __len__(self):\n        return self.num_batches\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return self.next()\n    \n    def reset(self):\n        self.idx = 0\n        random.shuffle(self.data_lis)\n\n    def next(self):\n        if self.idx >= self.data_num:\n            raise StopIteration\n        index = self.indices[self.idx : self.idx + self.batch_size]\n        d = [self.data_lis[i] for i in index]\n        d = torch.tensor(d)\n\n        # 0 is prepended to d as start symbol\n        data = torch.cat([torch.zeros(len(index), 1, dtype=torch.int64), d], dim=1)\n        target = torch.cat([d, torch.zeros(len(index), 1, dtype=torch.int64)], dim=1)\n        \n        self.idx += self.batch_size\n        return data, target\n\n    def read_file(self, data_file):\n        l = np.loadtxt(data_file, dtype=np.int32)\n        \n        return l[:n_samples]\n\n\nclass DisDataIter:\n    \"\"\" Toy data iter to load digits \"\"\"\n\n    def __init__(self, real_data_file, fake_data_file, batch_size):\n        super(DisDataIter, self).__init__()\n        self.batch_size = batch_size\n        real_data_lis = self.read_file(real_data_file)\n        fake_data_lis = self.read_file(fake_data_file)\n        self.data = real_data_lis + fake_data_lis\n        self.labels = [1 for _ in range(len(real_data_lis))] +\\\n                        [0 for _ in range(len(fake_data_lis))]\n        self.pairs = list(zip(self.data, self.labels))\n        self.data_num = len(self.pairs)\n        self.indices = range(self.data_num)\n        self.num_batches = math.ceil(self.data_num / self.batch_size)\n        self.idx = 0\n        self.reset()\n\n    def __len__(self):\n        return self.num_batches\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return self.next()\n    \n    def reset(self):\n        self.idx = 0\n        random.shuffle(self.pairs)\n\n    def next(self):\n        if self.idx >= self.data_num:\n            raise StopIteration\n        index = self.indices[self.idx : self.idx + self.batch_size]\n        pairs = [self.pairs[i] for i in index]\n        data = [p[0] for p in pairs]\n        label = [p[1] for p in pairs]\n        data = torch.tensor(data)\n        label = torch.tensor(label)\n        self.idx += self.batch_size\n        return data, label\n\n    def read_file(self, data_file):\n        l = np.loadtxt(data_file, dtype=np.int32)\n        \n        return l[:n_samples]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_samples(model, batch_size, generated_num, output_file):\n    samples = []\n    for _ in range(int(generated_num / batch_size)):\n        sample = model.sample(batch_size, g_seq_len).cpu().data.numpy().tolist()\n        samples.extend(sample)\n    with open(output_file, 'w') as fout:\n        for sample in samples:\n            string = ' '.join([str(s) for s in sample])\n            fout.write('{}\\n'.format(string))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class Generator(nn.Module):\n    \"\"\" Generator \"\"\"\n\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, use_cuda):\n        super(Generator, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.use_cuda = use_cuda\n        self.embed = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n        self.log_softmax = nn.LogSoftmax(dim=1)\n        self.init_params()\n\n    def forward(self, x):\n        \"\"\"\n        Embeds input and applies LSTM on the input sequence.\n        Inputs: x\n            - x: (batch_size, seq_len), sequence of tokens generated by generator\n        Outputs: out\n            - out: (batch_size * seq_len, vocab_size), lstm output prediction\n        \"\"\"\n        self.lstm.flatten_parameters()\n        h0, c0 = self.init_hidden(x.size(0))\n        emb = self.embed(x) # batch_size * seq_len * emb_dim \n        out, _ = self.lstm(emb, (h0, c0)) # out: batch_size * seq_len * hidden_dim\n        out = self.log_softmax(self.fc(out.contiguous().view(-1, self.hidden_dim))) # (batch_size*seq_len) * vocab_size\n        return out\n\n    def step(self, x, h, c):\n        \"\"\"\n        Embeds input and applies LSTM one token at a time (seq_len = 1).\n        Inputs: x, h, c\n            - x: (batch_size, 1), sequence of tokens generated by generator\n            - h: (1, batch_size, hidden_dim), lstm hidden state\n            - c: (1, batch_size, hidden_dim), lstm cell state\n        Outputs: out, h, c\n            - out: (batch_size, vocab_size), lstm output prediction\n            - h: (1, batch_size, hidden_dim), lstm hidden state\n            - c: (1, batch_size, hidden_dim), lstm cell state \n        \"\"\"\n        self.lstm.flatten_parameters()\n        emb = self.embed(x)# batch_size * 1 * emb_dim\n        out, (h, c) = self.lstm(emb, (h, c)) # out: batch_size * 1 * hidden_dim\n        out = self.log_softmax(self.fc(out.contiguous().view(-1, self.hidden_dim))) # batch_size * vocab_size\n        return out, h, c\n\n    def init_hidden(self, batch_size):\n        h = torch.zeros(1, batch_size, self.hidden_dim)\n        c = torch.zeros(1, batch_size, self.hidden_dim)\n        if self.use_cuda:\n            h, c = h.cuda(), c.cuda()\n        return h, c\n    \n    def init_params(self):\n        for param in self.parameters():\n            param.data.uniform_(-0.05, 0.05)\n\n    def sample(self, batch_size, seq_len, x=None):\n        \"\"\"\n        Samples the network and returns a batch of samples of length seq_len.\n        Outputs: out\n            - out: (batch_size * seq_len)\n        \"\"\"\n        samples = []\n        if x is None:\n            h, c = self.init_hidden(batch_size)\n            x = torch.zeros(batch_size, 1, dtype=torch.int64)\n            if self.use_cuda:\n                x = x.cuda()\n            for _ in range(seq_len):\n                out, h, c = self.step(x, h, c)\n                prob = torch.exp(out)\n                x = torch.multinomial(prob, 1)\n                samples.append(x)\n        else:\n            h, c = self.init_hidden(x.size(0))\n            given_len = x.size(1)\n            lis = x.chunk(x.size(1), dim=1)\n            for i in range(given_len):\n                out, h, c = self.step(lis[i], h, c)\n                samples.append(lis[i])\n            prob = torch.exp(out)\n            x = torch.multinomial(prob, 1)\n            for _ in range(given_len, seq_len):\n                samples.append(x)\n                out, h, c = self.step(x, h, c)\n                prob = torch.exp(out)\n                x = torch.multinomial(prob, 1)\n        out = torch.cat(samples, dim=1) # along the batch_size dimension\n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    \"\"\"\n    A CNN for text classification.\n    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n    Highway architecture based on the pooled feature maps is added. Dropout is adopted.\n    \"\"\"\n\n    def __init__(self, num_classes, vocab_size, embedding_dim, filter_sizes, num_filters, dropout_prob):\n        super(Discriminator, self).__init__()\n        self.embed = nn.Embedding(vocab_size, embedding_dim)#(256, 64)\n        self.convs = nn.ModuleList([\n            nn.Conv2d(1, num_f, (f_size, embedding_dim)) for f_size, num_f in zip(filter_sizes, num_filters)\n        ])\n        self.highway = nn.Linear(sum(num_filters), sum(num_filters))\n        self.dropout = nn.Dropout(p = dropout_prob)\n        self.fc = nn.Linear(sum(num_filters), num_classes)\n\n    def forward(self, x):\n        \"\"\"\n        Inputs: x\n            - x: (batch_size, seq_len)\n        Outputs: out\n            - out: (batch_size, num_classes)\n        \"\"\"\n        print(\"x.size() = {}\".format(x.size()))#torch.Size([64, 32])\n        print(\"x = {}\".format(x))\n        emb = self.embed(x).unsqueeze(1) # batch_size, 1 * seq_len * emb_dim\n        print(\"emb.size() = {}\".format(emb.size())) #torch.Size([64, 1, 32, 64])\n        print(emb)\n        print(\"emb = {}\".format(emb)) \n        for conv in self.convs:\n#             d_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n#             d_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n\n            conv = conv(emb)\n            print(\"conv(emb) = {}\".format(conv))\n            print(\"F.relu(conv)= {}\".format(f.relu(conv)))\n        convs = [F.relu(conv(emb)).squeeze(3) for conv in self.convs] # [batch_size * num_filter * seq_len]\n        pools = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in convs] # [batch_size * num_filter]\n        out = torch.cat(pools, 1)  # batch_size * sum(num_filters)\n        highway = self.highway(out)\n        transform = F.sigmoid(highway)\n        out = transform * F.relu(highway) + (1. - transform) * out # sets C = 1 - T\n        out = F.log_softmax(self.fc(self.dropout(out)), dim=1) # batch * num_classes\n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM","metadata":{}},{"cell_type":"code","source":"class TargetLSTM(nn.Module):\n    \"\"\" Target LSTM \"\"\"\n\n    def __init__(self,  vocab_size, embedding_dim, hidden_dim, use_cuda):\n        super(TargetLSTM, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.use_cuda = use_cuda\n        self.embed = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n        self.log_softmax = nn.LogSoftmax(dim=1)\n        self.init_params()\n\n    def forward(self, x):\n        \"\"\"\n        Embeds input and applies LSTM on the input sequence.\n\n        Inputs: x\n            - x: (batch_size, seq_len), sequence of tokens generated by generator\n        Outputs: out\n            - out: (batch_size, vocab_size), lstm output prediction\n        \"\"\"\n        self.lstm.flatten_parameters()\n        h0, c0 = self.init_hidden(x.size(0))\n        emb = self.embed(x) # batch_size * seq_len * emb_dim \n        out, _ = self.lstm(emb, (h0, c0)) # out: seq_len * batch_size * hidden_dim\n        out = self.log_softmax(self.fc(out.contiguous().view(-1, self.hidden_dim))) # seq_len * batch_size * vocab_size\n        return out\n\n    def step(self, x, h, c):\n        \"\"\"\n        Embeds input and applies LSTM one token at a time (seq_len = 1).\n\n        Inputs: x, h, c\n            - x: (batch_size, 1), sequence of tokens generated by generator\n            - h: (1, batch_size, hidden_dim), lstm hidden state\n            - c: (1, batch_size, hidden_dim), lstm cell state\n        Outputs: out, h, c\n            - out: (batch_size, 1, vocab_size), lstm output prediction\n            - h: (1, batch_size, hidden_dim), lstm hidden state\n            - c: (1, batch_size, hidden_dim), lstm cell state \n        \"\"\"\n        self.lstm.flatten_parameters()\n        emb = self.embed(x) # batch_size * 1 * emb_dim\n        out, (h, c) = self.lstm(emb, (h, c)) # out: batch_size * 1 * hidden_dim\n        out = self.log_softmax(self.fc(out.contiguous().view(-1, self.hidden_dim))) # batch_size * vocab_size\n        return out, h, c\n\n    def init_hidden(self, batch_size):\n        h = torch.zeros((1, batch_size, self.hidden_dim))\n        c = torch.zeros((1, batch_size, self.hidden_dim))\n        if self.use_cuda:\n            h, c = h.cuda(), c.cuda()\n        return h, c\n    \n    def init_params(self):\n        for param in self.parameters():\n            param.data.normal_(0, 1)\n\n    def sample(self, batch_size, seq_len):\n        \"\"\"\n        Samples the network and returns a batch of samples of length seq_len.\n\n        Outputs: out\n            - out: (batch_size * seq_len)\n        \"\"\"\n        samples = []\n        h, c = self.init_hidden(batch_size)\n        x = torch.zeros(batch_size, 1, dtype=torch.int64)\n        if self.use_cuda:\n            x = x.cuda()\n        for _ in range(seq_len):\n            out, h, c = self.step(x, h, c)\n            prob = torch.exp(out)\n            x = torch.multinomial(prob, 1)\n            samples.append(x)\n        out = torch.cat(samples, dim=1) # along the batch_size dimension\n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Rollout","metadata":{}},{"cell_type":"code","source":"class Rollout(object):\n    \"\"\" Rollout Policy \"\"\"\n\n    def __init__(self, model, update_rate):\n        self.ori_model = model\n        self.own_model = copy.deepcopy(model)\n        self.update_rate = update_rate\n\n    def get_reward(self, x, num, discriminator):\n        \"\"\"\n        Inputs: x, num, discriminator\n            - x: (batch_size, seq_len) input data\n            - num: rollout number\n            - discriminator: discrimanator model\n        \"\"\"\n        rewards = []\n        batch_size = x.size(0)\n        seq_len = x.size(1)\n        for i in range(num):\n            for l in range(1, seq_len):\n                data = x[:, 0:l]\n                samples = self.own_model.sample(batch_size, seq_len, data)\n                pred = discriminator(samples)\n                pred = pred.cpu().data[:,1].numpy()\n                if i == 0:\n                    rewards.append(pred)\n                else:\n                    rewards[l-1] += pred\n\n            # for the last token\n            pred = discriminator(x)\n            pred = pred.cpu().data[:, 1].numpy()\n            if i == 0:\n                rewards.append(pred)\n            else:\n                rewards[seq_len-1] += pred\n        rewards = np.transpose(np.array(rewards)) / (1.0 * num) # batch_size * seq_len\n        return rewards\n\n    def update_params(self):\n        dic = {}\n        for name, param in self.ori_model.named_parameters():\n            dic[name] = param.data\n        for name, param in self.own_model.named_parameters():\n            if name.startswith('emb'):\n                param.data = dic[name]\n            else:\n                param.data = self.update_rate * param.data + (1 - self.update_rate) * dic[name]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lossfunc","metadata":{}},{"cell_type":"code","source":"class PGLoss(nn.Module):\n    \"\"\"\n    Pseudo-loss that gives corresponding policy gradients (on calling .backward()) \n    for adversial training of Generator\n    \"\"\"\n\n    def __init__(self):\n        super(PGLoss, self).__init__()\n\n    def forward(self, pred, target, reward):\n        \"\"\"\n        Inputs: pred, target, reward\n            - pred: (batch_size, seq_len), \n            - target : (batch_size, seq_len), \n            - reward : (batch_size, ), reward of each whole sentence\n        \"\"\"\n        one_hot = torch.zeros(pred.size(), dtype=torch.uint8)\n        if pred.is_cuda:\n            one_hot = one_hot.cuda()\n        one_hot.scatter_(1, target.data.view(-1, 1), 1)\n        loss = torch.masked_select(pred, one_hot)\n        loss = loss * reward.contiguous().view(-1)\n        loss = -torch.sum(loss)\n        return loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pretrain","metadata":{}},{"cell_type":"code","source":"def pretrain_gene(gen, data_iter, criterion, \n                  optimizer, epochs, gen_pretrain_train_loss):\n    print(\"Pretrain Generator\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    for epoch in range(epochs):\n        total_loss = 0.\n        for data, target in data_iter:\n#             if cuda:\n#                 data, target = data.cuda(), target.cuda()\n            data, target = data.to(device), target.to(device)\n            target = target.contiguous().view(-1)\n#             print(\"--------\")\n#             print(\"target.size() = {}\".format(target.size()))\n#             print(\"target = {}\".format(target))\n#             print(\"data.size() = {}\".format(data.size()))\n#             print(\"data = {}\".format(data))\n            output = gen(data)\n            loss = criterion(output, target)\n            total_loss += loss.item()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        data_iter.reset()\n    avg_loss = total_loss / len(data_iter)\n    print(\"Epoch {}, train loss: {:.5f}\".format(epoch, avg_loss))\n    gen_pretrain_train_loss.append(avg_loss)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Eval","metadata":{}},{"cell_type":"code","source":"def eval_gene(model, data_iter, criterion):\n    \"\"\"\n    Evaluate generator with NLL\n    \"\"\"\n    total_loss = 0.\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    with torch.no_grad():\n        for data, target in data_iter:\n#             if torch.cuda.is_available():\n#                 data, target = data.to(\"cuda\"), target.to(\"cuda\")\n            data, target = data.to(device).long(), target.to(device).long()\n            target = target.contiguous().view(-1)\n            pred = model(data)\n            loss = criterion(pred, target)\n            total_loss += loss.item()\n    avg_loss = total_loss / len(data_iter)\n    return avg_loss\n\ndef eval_disc(model, data_iter, criterion):\n    \"\"\"\n    Evaluate discriminator, dropout is enabled\n    \"\"\"\n    correct = 0\n    total_loss = 0.\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    with torch.no_grad():\n        for data, target in data_iter:\n#             if  cuda:\n#                 data, target = data.cuda(), target.cuda()\n            data, target = data.to(device).long(), target.to(device).long()\n            target = target.contiguous().view(-1)\n            output = model(data)\n            pred = output.data.max(1)[1]\n            correct += pred.eq(target.data).cpu().sum()\n            loss = criterion(output, target)\n            total_loss += loss.item()\n    avg_loss = total_loss / len(data_iter)\n    acc = correct.item() / data_iter.data_num\n    return avg_loss, acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main train","metadata":{}},{"cell_type":"code","source":"def train_gene(gen, dis, rollout, pg_loss, optimizer, epochs):\n    \"\"\"\n    Train generator with the guidance of policy gradient\n    \"\"\"\n    for epoch in range(epochs):\n        # construct the input to the genrator, add zeros before samples and delete the last column\n        samples = generator.sample(batch_size, g_seq_len)\n        zeros = torch.zeros(batch_size, 1, dtype=torch.int64)\n        if samples.is_cuda:\n            zeros = zeros.cuda()\n        inputs = torch.cat([zeros, samples.data], dim = 1)[:, :-1].contiguous()\n        targets = samples.data.contiguous().view((-1,))\n\n        # calculate the reward\n        rewards = torch.tensor(rollout.get_reward(samples, n_rollout, dis))\n        if args.cuda:\n            rewards = rewards.cuda()\n\n        # update generator\n        output = gen(inputs)\n        loss = pg_loss(output, targets, rewards)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \ndef train_disc(dis, gen, criterion, optimizer, epochs, \n        dis_adversarial_train_loss, dis_adversarial_train_acc):\n    \n    generate_samples(gen,  batch_size,  n_samples, NEGATIVE_FILE)\n    data_iter = DisDataIter(POSITIVE_FILE, NEGATIVE_FILE,  batch_size)\n    \n    for epoch in range(epochs):\n        correct = 0\n        total_loss = 0.\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        for data, target in data_iter:\n#             if  cuda:\n#                 data, target = data.cuda(), target.cuda()\n            data, target = data.to(device).long(), target.to(device).long()\n            target = target.contiguous().view(-1)\n            output = dis(data)\n            pred = output.data.max(1)[1]\n            correct += pred.eq(target.data).cpu().sum()\n            loss = criterion(output, target)\n            total_loss += loss.item()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        data_iter.reset()\n        avg_loss = total_loss / len(data_iter)\n        acc = correct.item() / data_iter.data_num\n        print(\"Epoch {}, train loss: {:.5f}, train acc: {:.3f}\".format(epoch, avg_loss, acc))\n        dis_adversarial_train_loss.append(avg_loss)\n        dis_adversarial_train_acc.append(acc)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def adversarial_train(gen, dis, rollout, pg_loss, nll_loss, gen_optimizer, dis_optimizer, \n        dis_adversarial_train_loss, dis_adversarial_train_acc):\n    \"\"\"\n    Adversarially train generator and discriminator\n    \"\"\"\n    # train generator for g_steps\n    print(\"#Train generator\")\n    for i in range( g_steps):\n        print(\"##G-Step {}\".format(i))\n        train_gene(gen, dis, rollout, pg_loss, gen_optimizer,  gk_epochs,  )\n\n    # train discriminator for d_steps\n    print(\"#Train discriminator\")\n    for i in range( d_steps):\n        print(\"##D-Step {}\".format(i))\n        train_disc(dis, gen, nll_loss, dis_optimizer,  dk_epochs, \n            dis_adversarial_train_loss, dis_adversarial_train_acc,  )\n\n    # update roll-out model\n    rollout.update_params()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# l = np.loadtxt(\"./gene.data\")\n# len(l)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"code","source":"# Parse arguments\n\ncuda = torch.cuda.is_available()\ntorch.manual_seed(seed)\nif cuda:\n    torch.cuda.manual_seed(seed)\n# if not hpc:\n#      data_path = ''\n# POSITIVE_FILE =  data_path + POSITIVE_FILE\n# NEGATIVE_FILE =  data_path + NEGATIVE_FILE\nPOSITIVE_FILE = os.path.join(data_path ,POSITIVE_FILE)\nNEGATIVE_FILE = NEGATIVE_FILE\n# Set models, criteria, optimizers\ngenerator = Generator( vocab_size, g_embed_dim, g_hidden_dim,  cuda)\ndiscriminator = Discriminator(d_num_class,  vocab_size, d_embed_dim, d_filter_sizes, d_num_filters, d_dropout_prob)\ntarget_lstm = TargetLSTM( vocab_size, g_embed_dim, g_hidden_dim,  cuda)\nnll_loss = nn.NLLLoss()\npg_loss = PGLoss()\nif cuda:\n    generator = generator.cuda()\n    discriminator = discriminator.cuda()\n    target_lstm = target_lstm.cuda()\n    nll_loss = nll_loss.cuda()\n    pg_loss = pg_loss.cuda()\n    cudnn.benchmark = True\ngen_optimizer = optim.Adam(params=generator.parameters(), lr= gen_lr)\ndis_optimizer = optim.SGD(params=discriminator.parameters(), lr= dis_lr)\n\n# Container of experiment data\ngen_pretrain_train_loss = []\ngen_pretrain_eval_loss = []\ndis_pretrain_train_loss = []\ndis_pretrain_train_acc = []\ndis_pretrain_eval_loss = []\ndis_pretrain_eval_acc = []\ngen_adversarial_eval_loss = []\ndis_adversarial_train_loss = []\ndis_adversarial_train_acc = []\ndis_adversarial_eval_loss = []\ndis_adversarial_eval_acc = []\n'''\n# Generate toy data using target LSTM\nprint('#####################################################')\nprint('Generating data ...')\nprint('#####################################################\\n\\n')\n# generate_samples(target_lstm,  batch_size,  n_samples, POSITIVE_FILE)\nprint(\"POSITIVE_FILE = {}\".format(POSITIVE_FILE))\nprint(\"NEGATIVE_FILE = {}\".format(NEGATIVE_FILE))\n# Pre-train generator using MLE\nprint('#####################################################')\nprint('Start pre-training generator with MLE...')\nprint('#####################################################\\n')\ngen_data_iter = GenDataIter(POSITIVE_FILE,  batch_size)\nfor i in range( g_pretrain_steps):\n    print(\"G-Step {}\".format(i))\n    pretrain_gene(generator, gen_data_iter, nll_loss, \n        gen_optimizer,  gk_epochs, gen_pretrain_train_loss)\n    generate_samples(generator,  batch_size,  n_samples, NEGATIVE_FILE)\n    eval_iter = GenDataIter(NEGATIVE_FILE,  batch_size)\n    gen_loss = eval_gene(target_lstm, eval_iter, nll_loss)\n    gen_pretrain_eval_loss.append(gen_loss)\n    print(\"eval loss: {:.5f}\\n\".format(gen_loss))\nprint('#####################################################\\n\\n')\n'''\n# Pre-train discriminator\nprint('#####################################################')\nprint('Start pre-training discriminator...')\nprint('#####################################################\\n')\nfor i in range( d_pretrain_steps):\n    print(\"D-Step {}\".format(i))\n    train_disc(discriminator, generator, nll_loss, \n        dis_optimizer,  dk_epochs, dis_adversarial_train_loss, dis_adversarial_train_acc)\n    generate_samples(generator,  batch_size,  n_samples, NEGATIVE_FILE)\n    eval_iter = DisDataIter(POSITIVE_FILE, NEGATIVE_FILE,  batch_size)\n    dis_loss, dis_acc = eval_disc(discriminator, eval_iter, nll_loss)\n    dis_pretrain_eval_loss.append(dis_loss)\n    dis_pretrain_eval_acc.append(dis_acc)\n    print(\"eval loss: {:.5f}, eval acc: {:.3f}\\n\".format(dis_loss, dis_acc))\nprint('#####################################################\\n\\n')\n\n# Adversarial training\nprint('#####################################################')\nprint('Start adversarial training...')\nprint('#####################################################\\n')\nrollout = Rollout(generator,  update_rate)\nfor i in range(rounds):\n    print(\"Round {}\".format(i))\n    adversarial_train(generator, discriminator, rollout, \n        pg_loss, nll_loss, gen_optimizer, dis_optimizer, \n        dis_adversarial_train_loss, dis_adversarial_train_acc)\n    generate_samples(generator,  batch_size,  n_samples, NEGATIVE_FILE)\n    gen_eval_iter = GenDataIter(NEGATIVE_FILE,  batch_size)\n    dis_eval_iter = DisDataIter(POSITIVE_FILE, NEGATIVE_FILE,  batch_size)\n    gen_loss = eval_gene(target_lstm, gen_eval_iter, nll_loss)\n    gen_adversarial_eval_loss.append(gen_loss)\n    dis_loss, dis_acc = eval_disc(discriminator, dis_eval_iter, nll_loss)\n    dis_adversarial_eval_loss.append(dis_loss)\n    dis_adversarial_eval_acc.append(dis_acc)\n    print(\"gen eval loss: {:.5f}, dis eval loss: {:.5f}, dis eval acc: {:.3f}\\n\"\n        .format(gen_loss, dis_loss, dis_acc))\nprint('#####################################################')\nprint('Model Save...')\nprint('#####################################################\\n')\n    \ntorch.save(generator.state_dict(), 'generator.pth')   \ntorch.save(discriminator.state_dict(), 'discriminator.pth')\ntorch.save(target_lstm.state_dict(), 'target_lstm.pth')\nprint(\"complete\")\n\n# Save experiment data\nwith open('experiment.pkl', 'wb') as f:\n    pkl.dump(\n        (gen_pretrain_train_loss,\n            gen_pretrain_eval_loss,\n            dis_pretrain_train_loss,\n            dis_pretrain_train_acc,\n            dis_pretrain_eval_loss,\n            dis_pretrain_eval_acc,\n            gen_adversarial_eval_loss,\n            dis_adversarial_train_loss,\n            dis_adversarial_train_acc,\n            dis_adversarial_eval_loss,\n            dis_adversarial_eval_acc),\n        f,\n        protocol=pkl.HIGHEST_PROTOCOL\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}